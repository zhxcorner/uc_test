 -> DualBranchVSSM(
  (patch_embed): PatchEmbed2D(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): VSSLayer(
      (blocks): ModuleList(
        (0): SS_Conv_SSM(
          (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=48, out_features=192, bias=False)
            (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
            (act): SiLU()
            (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=96, out_features=48, bias=False)
          )
          (drop_path): timm.DropPath(0.0)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
        (1): SS_Conv_SSM(
          (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=48, out_features=192, bias=False)
            (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
            (act): SiLU()
            (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=96, out_features=48, bias=False)
          )
          (drop_path): timm.DropPath(0.011111111380159855)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
      )
      (downsample): PatchMerging2D(
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): VSSLayer(
      (blocks): ModuleList(
        (0): SS_Conv_SSM(
          (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=96, out_features=384, bias=False)
            (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
            (act): SiLU()
            (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=192, out_features=96, bias=False)
          )
          (drop_path): timm.DropPath(0.02222222276031971)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
        (1): SS_Conv_SSM(
          (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=96, out_features=384, bias=False)
            (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
            (act): SiLU()
            (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=192, out_features=96, bias=False)
          )
          (drop_path): timm.DropPath(0.03333333507180214)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
      )
      (downsample): PatchMerging2D(
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): VSSLayer(
      (blocks): ModuleList(
        (0): SS_Conv_SSM(
          (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=192, out_features=768, bias=False)
            (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
            (act): SiLU()
            (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=384, out_features=192, bias=False)
          )
          (drop_path): timm.DropPath(0.04444444552063942)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
        (1): SS_Conv_SSM(
          (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=192, out_features=768, bias=False)
            (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
            (act): SiLU()
            (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=384, out_features=192, bias=False)
          )
          (drop_path): timm.DropPath(0.0555555559694767)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
        (2): SS_Conv_SSM(
          (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=192, out_features=768, bias=False)
            (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
            (act): SiLU()
            (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=384, out_features=192, bias=False)
          )
          (drop_path): timm.DropPath(0.06666667014360428)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
        (3): SS_Conv_SSM(
          (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=192, out_features=768, bias=False)
            (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
            (act): SiLU()
            (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=384, out_features=192, bias=False)
          )
          (drop_path): timm.DropPath(0.07777778059244156)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
      )
      (downsample): PatchMerging2D(
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): VSSLayer(
      (blocks): ModuleList(
        (0): SS_Conv_SSM(
          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=384, out_features=1536, bias=False)
            (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            (act): SiLU()
            (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=768, out_features=384, bias=False)
          )
          (drop_path): timm.DropPath(0.08888889104127884)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
        (1): SS_Conv_SSM(
          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): SS2D(
            (in_proj): Linear(in_features=384, out_features=1536, bias=False)
            (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            (act): SiLU()
            (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (out_proj): Linear(in_features=768, out_features=384, bias=False)
          )
          (drop_path): timm.DropPath(0.10000000149011612)
          (conv33conv33conv11): Sequential(
            (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU()
            (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (8): ReLU()
          )
        )
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=768, out_features=2, bias=True)
  (edge_extractor): SobelConv(
    (sobel_conv_y): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)
    (sobel_conv_x): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)
  )
  (edge_pools): ModuleList(
    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    (1): MaxPool2d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)
    (2): MaxPool2d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
  )
  (edge_convs): ModuleList(
    (0): Conv(
      (conv): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): SiLU()
    )
    (1): Conv(
      (conv): Conv2d(3, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): SiLU()
    )
    (2): Conv(
      (conv): Conv2d(3, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): SiLU()
    )
  )
  (fusers): ModuleList(
    (0): GatedConvEdgeFusion(
      (align_main): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (align_edge): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (gate): Sequential(
        (0): Conv2d(192, 2, kernel_size=(1, 1), stride=(1, 1))
        (1): Softmax(dim=1)
      )
      (output_proj): Sequential(
        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): GatedConvEdgeFusion(
      (align_main): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      (align_edge): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      (gate): Sequential(
        (0): Conv2d(384, 2, kernel_size=(1, 1), stride=(1, 1))
        (1): Softmax(dim=1)
      )
      (output_proj): Sequential(
        (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (2): GatedConvEdgeFusion(
      (align_main): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (align_edge): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (gate): Sequential(
        (0): Conv2d(768, 2, kernel_size=(1, 1), stride=(1, 1))
        (1): Softmax(dim=1)
      )
      (output_proj): Sequential(
        (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
)
patch_embed -> PatchEmbed2D(
  (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
)
patch_embed.proj -> Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
patch_embed.norm -> LayerNorm((96,), eps=1e-05, elementwise_affine=True)
pos_drop -> Dropout(p=0.0, inplace=False)
layers -> ModuleList(
  (0): VSSLayer(
    (blocks): ModuleList(
      (0): SS_Conv_SSM(
        (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=48, out_features=192, bias=False)
          (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (act): SiLU()
          (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=96, out_features=48, bias=False)
        )
        (drop_path): timm.DropPath(0.0)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
      (1): SS_Conv_SSM(
        (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=48, out_features=192, bias=False)
          (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (act): SiLU()
          (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=96, out_features=48, bias=False)
        )
        (drop_path): timm.DropPath(0.011111111380159855)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
    )
    (downsample): PatchMerging2D(
      (reduction): Linear(in_features=384, out_features=192, bias=False)
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
  )
  (1): VSSLayer(
    (blocks): ModuleList(
      (0): SS_Conv_SSM(
        (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=96, out_features=384, bias=False)
          (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (act): SiLU()
          (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=192, out_features=96, bias=False)
        )
        (drop_path): timm.DropPath(0.02222222276031971)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
      (1): SS_Conv_SSM(
        (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=96, out_features=384, bias=False)
          (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (act): SiLU()
          (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=192, out_features=96, bias=False)
        )
        (drop_path): timm.DropPath(0.03333333507180214)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
    )
    (downsample): PatchMerging2D(
      (reduction): Linear(in_features=768, out_features=384, bias=False)
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (2): VSSLayer(
    (blocks): ModuleList(
      (0): SS_Conv_SSM(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (act): SiLU()
          (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (drop_path): timm.DropPath(0.04444444552063942)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
      (1): SS_Conv_SSM(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (act): SiLU()
          (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (drop_path): timm.DropPath(0.0555555559694767)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
      (2): SS_Conv_SSM(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (act): SiLU()
          (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (drop_path): timm.DropPath(0.06666667014360428)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
      (3): SS_Conv_SSM(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (act): SiLU()
          (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (drop_path): timm.DropPath(0.07777778059244156)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
    )
    (downsample): PatchMerging2D(
      (reduction): Linear(in_features=1536, out_features=768, bias=False)
      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    )
  )
  (3): VSSLayer(
    (blocks): ModuleList(
      (0): SS_Conv_SSM(
        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (act): SiLU()
          (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
        (drop_path): timm.DropPath(0.08888889104127884)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
      (1): SS_Conv_SSM(
        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): SS2D(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (act): SiLU()
          (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
        (drop_path): timm.DropPath(0.10000000149011612)
        (conv33conv33conv11): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (8): ReLU()
        )
      )
    )
  )
)
layers.0 -> VSSLayer(
  (blocks): ModuleList(
    (0): SS_Conv_SSM(
      (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=48, out_features=192, bias=False)
        (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (act): SiLU()
        (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=96, out_features=48, bias=False)
      )
      (drop_path): timm.DropPath(0.0)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
    (1): SS_Conv_SSM(
      (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=48, out_features=192, bias=False)
        (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (act): SiLU()
        (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=96, out_features=48, bias=False)
      )
      (drop_path): timm.DropPath(0.011111111380159855)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
  )
  (downsample): PatchMerging2D(
    (reduction): Linear(in_features=384, out_features=192, bias=False)
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
)
layers.0.blocks -> ModuleList(
  (0): SS_Conv_SSM(
    (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=48, out_features=192, bias=False)
      (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
      (act): SiLU()
      (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=96, out_features=48, bias=False)
    )
    (drop_path): timm.DropPath(0.0)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
  (1): SS_Conv_SSM(
    (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=48, out_features=192, bias=False)
      (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
      (act): SiLU()
      (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=96, out_features=48, bias=False)
    )
    (drop_path): timm.DropPath(0.011111111380159855)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
)
layers.0.blocks.0 -> SS_Conv_SSM(
  (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=48, out_features=192, bias=False)
    (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
    (act): SiLU()
    (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=96, out_features=48, bias=False)
  )
  (drop_path): timm.DropPath(0.0)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.0.blocks.0.ln_1 -> LayerNorm((48,), eps=1e-05, elementwise_affine=True)
layers.0.blocks.0.self_attention -> SS2D(
  (in_proj): Linear(in_features=48, out_features=192, bias=False)
  (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
  (act): SiLU()
  (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=96, out_features=48, bias=False)
)
layers.0.blocks.0.self_attention.in_proj -> Linear(in_features=48, out_features=192, bias=False)
layers.0.blocks.0.self_attention.conv2d -> Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
layers.0.blocks.0.self_attention.act -> SiLU()
layers.0.blocks.0.self_attention.out_norm -> LayerNorm((96,), eps=1e-05, elementwise_affine=True)
layers.0.blocks.0.self_attention.out_proj -> Linear(in_features=96, out_features=48, bias=False)
layers.0.blocks.0.drop_path -> timm.DropPath(0.0)
layers.0.blocks.0.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.0.blocks.0.conv33conv33conv11.0 -> BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.0.blocks.0.conv33conv33conv11.1 -> Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.0.blocks.0.conv33conv33conv11.2 -> BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.0.blocks.0.conv33conv33conv11.3 -> ReLU()
layers.0.blocks.0.conv33conv33conv11.4 -> Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.0.blocks.0.conv33conv33conv11.5 -> BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.0.blocks.0.conv33conv33conv11.6 -> ReLU()
layers.0.blocks.0.conv33conv33conv11.7 -> Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
layers.0.blocks.0.conv33conv33conv11.8 -> ReLU()
layers.0.blocks.1 -> SS_Conv_SSM(
  (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=48, out_features=192, bias=False)
    (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
    (act): SiLU()
    (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=96, out_features=48, bias=False)
  )
  (drop_path): timm.DropPath(0.011111111380159855)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.0.blocks.1.ln_1 -> LayerNorm((48,), eps=1e-05, elementwise_affine=True)
layers.0.blocks.1.self_attention -> SS2D(
  (in_proj): Linear(in_features=48, out_features=192, bias=False)
  (conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
  (act): SiLU()
  (out_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=96, out_features=48, bias=False)
)
layers.0.blocks.1.self_attention.in_proj -> Linear(in_features=48, out_features=192, bias=False)
layers.0.blocks.1.self_attention.conv2d -> Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
layers.0.blocks.1.self_attention.act -> SiLU()
layers.0.blocks.1.self_attention.out_norm -> LayerNorm((96,), eps=1e-05, elementwise_affine=True)
layers.0.blocks.1.self_attention.out_proj -> Linear(in_features=96, out_features=48, bias=False)
layers.0.blocks.1.drop_path -> timm.DropPath(0.011111111380159855)
layers.0.blocks.1.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.0.blocks.1.conv33conv33conv11.0 -> BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.0.blocks.1.conv33conv33conv11.1 -> Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.0.blocks.1.conv33conv33conv11.2 -> BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.0.blocks.1.conv33conv33conv11.3 -> ReLU()
layers.0.blocks.1.conv33conv33conv11.4 -> Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.0.blocks.1.conv33conv33conv11.5 -> BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.0.blocks.1.conv33conv33conv11.6 -> ReLU()
layers.0.blocks.1.conv33conv33conv11.7 -> Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
layers.0.blocks.1.conv33conv33conv11.8 -> ReLU()
layers.0.downsample -> PatchMerging2D(
  (reduction): Linear(in_features=384, out_features=192, bias=False)
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
)
layers.0.downsample.reduction -> Linear(in_features=384, out_features=192, bias=False)
layers.0.downsample.norm -> LayerNorm((384,), eps=1e-05, elementwise_affine=True)
layers.1 -> VSSLayer(
  (blocks): ModuleList(
    (0): SS_Conv_SSM(
      (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=96, out_features=384, bias=False)
        (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (act): SiLU()
        (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=192, out_features=96, bias=False)
      )
      (drop_path): timm.DropPath(0.02222222276031971)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
    (1): SS_Conv_SSM(
      (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=96, out_features=384, bias=False)
        (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (act): SiLU()
        (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=192, out_features=96, bias=False)
      )
      (drop_path): timm.DropPath(0.03333333507180214)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
  )
  (downsample): PatchMerging2D(
    (reduction): Linear(in_features=768, out_features=384, bias=False)
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
layers.1.blocks -> ModuleList(
  (0): SS_Conv_SSM(
    (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=96, out_features=384, bias=False)
      (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
      (act): SiLU()
      (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=192, out_features=96, bias=False)
    )
    (drop_path): timm.DropPath(0.02222222276031971)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
  (1): SS_Conv_SSM(
    (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=96, out_features=384, bias=False)
      (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
      (act): SiLU()
      (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=192, out_features=96, bias=False)
    )
    (drop_path): timm.DropPath(0.03333333507180214)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
)
layers.1.blocks.0 -> SS_Conv_SSM(
  (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=96, out_features=384, bias=False)
    (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
    (act): SiLU()
    (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=192, out_features=96, bias=False)
  )
  (drop_path): timm.DropPath(0.02222222276031971)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.1.blocks.0.ln_1 -> LayerNorm((96,), eps=1e-05, elementwise_affine=True)
layers.1.blocks.0.self_attention -> SS2D(
  (in_proj): Linear(in_features=96, out_features=384, bias=False)
  (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
  (act): SiLU()
  (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=192, out_features=96, bias=False)
)
layers.1.blocks.0.self_attention.in_proj -> Linear(in_features=96, out_features=384, bias=False)
layers.1.blocks.0.self_attention.conv2d -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
layers.1.blocks.0.self_attention.act -> SiLU()
layers.1.blocks.0.self_attention.out_norm -> LayerNorm((192,), eps=1e-05, elementwise_affine=True)
layers.1.blocks.0.self_attention.out_proj -> Linear(in_features=192, out_features=96, bias=False)
layers.1.blocks.0.drop_path -> timm.DropPath(0.02222222276031971)
layers.1.blocks.0.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.1.blocks.0.conv33conv33conv11.0 -> BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.1.blocks.0.conv33conv33conv11.1 -> Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.1.blocks.0.conv33conv33conv11.2 -> BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.1.blocks.0.conv33conv33conv11.3 -> ReLU()
layers.1.blocks.0.conv33conv33conv11.4 -> Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.1.blocks.0.conv33conv33conv11.5 -> BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.1.blocks.0.conv33conv33conv11.6 -> ReLU()
layers.1.blocks.0.conv33conv33conv11.7 -> Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
layers.1.blocks.0.conv33conv33conv11.8 -> ReLU()
layers.1.blocks.1 -> SS_Conv_SSM(
  (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=96, out_features=384, bias=False)
    (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
    (act): SiLU()
    (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=192, out_features=96, bias=False)
  )
  (drop_path): timm.DropPath(0.03333333507180214)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.1.blocks.1.ln_1 -> LayerNorm((96,), eps=1e-05, elementwise_affine=True)
layers.1.blocks.1.self_attention -> SS2D(
  (in_proj): Linear(in_features=96, out_features=384, bias=False)
  (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
  (act): SiLU()
  (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=192, out_features=96, bias=False)
)
layers.1.blocks.1.self_attention.in_proj -> Linear(in_features=96, out_features=384, bias=False)
layers.1.blocks.1.self_attention.conv2d -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
layers.1.blocks.1.self_attention.act -> SiLU()
layers.1.blocks.1.self_attention.out_norm -> LayerNorm((192,), eps=1e-05, elementwise_affine=True)
layers.1.blocks.1.self_attention.out_proj -> Linear(in_features=192, out_features=96, bias=False)
layers.1.blocks.1.drop_path -> timm.DropPath(0.03333333507180214)
layers.1.blocks.1.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.1.blocks.1.conv33conv33conv11.0 -> BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.1.blocks.1.conv33conv33conv11.1 -> Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.1.blocks.1.conv33conv33conv11.2 -> BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.1.blocks.1.conv33conv33conv11.3 -> ReLU()
layers.1.blocks.1.conv33conv33conv11.4 -> Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.1.blocks.1.conv33conv33conv11.5 -> BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.1.blocks.1.conv33conv33conv11.6 -> ReLU()
layers.1.blocks.1.conv33conv33conv11.7 -> Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
layers.1.blocks.1.conv33conv33conv11.8 -> ReLU()
layers.1.downsample -> PatchMerging2D(
  (reduction): Linear(in_features=768, out_features=384, bias=False)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
layers.1.downsample.reduction -> Linear(in_features=768, out_features=384, bias=False)
layers.1.downsample.norm -> LayerNorm((768,), eps=1e-05, elementwise_affine=True)
layers.2 -> VSSLayer(
  (blocks): ModuleList(
    (0): SS_Conv_SSM(
      (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (act): SiLU()
        (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (drop_path): timm.DropPath(0.04444444552063942)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
    (1): SS_Conv_SSM(
      (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (act): SiLU()
        (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (drop_path): timm.DropPath(0.0555555559694767)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
    (2): SS_Conv_SSM(
      (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (act): SiLU()
        (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (drop_path): timm.DropPath(0.06666667014360428)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
    (3): SS_Conv_SSM(
      (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (act): SiLU()
        (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (drop_path): timm.DropPath(0.07777778059244156)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
  )
  (downsample): PatchMerging2D(
    (reduction): Linear(in_features=1536, out_features=768, bias=False)
    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
)
layers.2.blocks -> ModuleList(
  (0): SS_Conv_SSM(
    (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=192, out_features=768, bias=False)
      (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      (act): SiLU()
      (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=384, out_features=192, bias=False)
    )
    (drop_path): timm.DropPath(0.04444444552063942)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
  (1): SS_Conv_SSM(
    (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=192, out_features=768, bias=False)
      (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      (act): SiLU()
      (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=384, out_features=192, bias=False)
    )
    (drop_path): timm.DropPath(0.0555555559694767)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
  (2): SS_Conv_SSM(
    (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=192, out_features=768, bias=False)
      (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      (act): SiLU()
      (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=384, out_features=192, bias=False)
    )
    (drop_path): timm.DropPath(0.06666667014360428)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
  (3): SS_Conv_SSM(
    (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=192, out_features=768, bias=False)
      (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      (act): SiLU()
      (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=384, out_features=192, bias=False)
    )
    (drop_path): timm.DropPath(0.07777778059244156)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
)
layers.2.blocks.0 -> SS_Conv_SSM(
  (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=192, out_features=768, bias=False)
    (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
    (act): SiLU()
    (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=384, out_features=192, bias=False)
  )
  (drop_path): timm.DropPath(0.04444444552063942)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.2.blocks.0.ln_1 -> LayerNorm((192,), eps=1e-05, elementwise_affine=True)
layers.2.blocks.0.self_attention -> SS2D(
  (in_proj): Linear(in_features=192, out_features=768, bias=False)
  (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
  (act): SiLU()
  (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=384, out_features=192, bias=False)
)
layers.2.blocks.0.self_attention.in_proj -> Linear(in_features=192, out_features=768, bias=False)
layers.2.blocks.0.self_attention.conv2d -> Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
layers.2.blocks.0.self_attention.act -> SiLU()
layers.2.blocks.0.self_attention.out_norm -> LayerNorm((384,), eps=1e-05, elementwise_affine=True)
layers.2.blocks.0.self_attention.out_proj -> Linear(in_features=384, out_features=192, bias=False)
layers.2.blocks.0.drop_path -> timm.DropPath(0.04444444552063942)
layers.2.blocks.0.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.2.blocks.0.conv33conv33conv11.0 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.0.conv33conv33conv11.1 -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.2.blocks.0.conv33conv33conv11.2 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.0.conv33conv33conv11.3 -> ReLU()
layers.2.blocks.0.conv33conv33conv11.4 -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.2.blocks.0.conv33conv33conv11.5 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.0.conv33conv33conv11.6 -> ReLU()
layers.2.blocks.0.conv33conv33conv11.7 -> Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
layers.2.blocks.0.conv33conv33conv11.8 -> ReLU()
layers.2.blocks.1 -> SS_Conv_SSM(
  (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=192, out_features=768, bias=False)
    (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
    (act): SiLU()
    (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=384, out_features=192, bias=False)
  )
  (drop_path): timm.DropPath(0.0555555559694767)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.2.blocks.1.ln_1 -> LayerNorm((192,), eps=1e-05, elementwise_affine=True)
layers.2.blocks.1.self_attention -> SS2D(
  (in_proj): Linear(in_features=192, out_features=768, bias=False)
  (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
  (act): SiLU()
  (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=384, out_features=192, bias=False)
)
layers.2.blocks.1.self_attention.in_proj -> Linear(in_features=192, out_features=768, bias=False)
layers.2.blocks.1.self_attention.conv2d -> Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
layers.2.blocks.1.self_attention.act -> SiLU()
layers.2.blocks.1.self_attention.out_norm -> LayerNorm((384,), eps=1e-05, elementwise_affine=True)
layers.2.blocks.1.self_attention.out_proj -> Linear(in_features=384, out_features=192, bias=False)
layers.2.blocks.1.drop_path -> timm.DropPath(0.0555555559694767)
layers.2.blocks.1.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.2.blocks.1.conv33conv33conv11.0 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.1.conv33conv33conv11.1 -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.2.blocks.1.conv33conv33conv11.2 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.1.conv33conv33conv11.3 -> ReLU()
layers.2.blocks.1.conv33conv33conv11.4 -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.2.blocks.1.conv33conv33conv11.5 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.1.conv33conv33conv11.6 -> ReLU()
layers.2.blocks.1.conv33conv33conv11.7 -> Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
layers.2.blocks.1.conv33conv33conv11.8 -> ReLU()
layers.2.blocks.2 -> SS_Conv_SSM(
  (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=192, out_features=768, bias=False)
    (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
    (act): SiLU()
    (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=384, out_features=192, bias=False)
  )
  (drop_path): timm.DropPath(0.06666667014360428)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.2.blocks.2.ln_1 -> LayerNorm((192,), eps=1e-05, elementwise_affine=True)
layers.2.blocks.2.self_attention -> SS2D(
  (in_proj): Linear(in_features=192, out_features=768, bias=False)
  (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
  (act): SiLU()
  (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=384, out_features=192, bias=False)
)
layers.2.blocks.2.self_attention.in_proj -> Linear(in_features=192, out_features=768, bias=False)
layers.2.blocks.2.self_attention.conv2d -> Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
layers.2.blocks.2.self_attention.act -> SiLU()
layers.2.blocks.2.self_attention.out_norm -> LayerNorm((384,), eps=1e-05, elementwise_affine=True)
layers.2.blocks.2.self_attention.out_proj -> Linear(in_features=384, out_features=192, bias=False)
layers.2.blocks.2.drop_path -> timm.DropPath(0.06666667014360428)
layers.2.blocks.2.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.2.blocks.2.conv33conv33conv11.0 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.2.conv33conv33conv11.1 -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.2.blocks.2.conv33conv33conv11.2 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.2.conv33conv33conv11.3 -> ReLU()
layers.2.blocks.2.conv33conv33conv11.4 -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.2.blocks.2.conv33conv33conv11.5 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.2.conv33conv33conv11.6 -> ReLU()
layers.2.blocks.2.conv33conv33conv11.7 -> Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
layers.2.blocks.2.conv33conv33conv11.8 -> ReLU()
layers.2.blocks.3 -> SS_Conv_SSM(
  (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=192, out_features=768, bias=False)
    (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
    (act): SiLU()
    (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=384, out_features=192, bias=False)
  )
  (drop_path): timm.DropPath(0.07777778059244156)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.2.blocks.3.ln_1 -> LayerNorm((192,), eps=1e-05, elementwise_affine=True)
layers.2.blocks.3.self_attention -> SS2D(
  (in_proj): Linear(in_features=192, out_features=768, bias=False)
  (conv2d): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
  (act): SiLU()
  (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=384, out_features=192, bias=False)
)
layers.2.blocks.3.self_attention.in_proj -> Linear(in_features=192, out_features=768, bias=False)
layers.2.blocks.3.self_attention.conv2d -> Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
layers.2.blocks.3.self_attention.act -> SiLU()
layers.2.blocks.3.self_attention.out_norm -> LayerNorm((384,), eps=1e-05, elementwise_affine=True)
layers.2.blocks.3.self_attention.out_proj -> Linear(in_features=384, out_features=192, bias=False)
layers.2.blocks.3.drop_path -> timm.DropPath(0.07777778059244156)
layers.2.blocks.3.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.2.blocks.3.conv33conv33conv11.0 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.3.conv33conv33conv11.1 -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.2.blocks.3.conv33conv33conv11.2 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.3.conv33conv33conv11.3 -> ReLU()
layers.2.blocks.3.conv33conv33conv11.4 -> Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.2.blocks.3.conv33conv33conv11.5 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.2.blocks.3.conv33conv33conv11.6 -> ReLU()
layers.2.blocks.3.conv33conv33conv11.7 -> Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
layers.2.blocks.3.conv33conv33conv11.8 -> ReLU()
layers.2.downsample -> PatchMerging2D(
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
)
layers.2.downsample.reduction -> Linear(in_features=1536, out_features=768, bias=False)
layers.2.downsample.norm -> LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
layers.3 -> VSSLayer(
  (blocks): ModuleList(
    (0): SS_Conv_SSM(
      (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=384, out_features=1536, bias=False)
        (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (act): SiLU()
        (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=768, out_features=384, bias=False)
      )
      (drop_path): timm.DropPath(0.08888889104127884)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
    (1): SS_Conv_SSM(
      (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (in_proj): Linear(in_features=384, out_features=1536, bias=False)
        (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (act): SiLU()
        (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (out_proj): Linear(in_features=768, out_features=384, bias=False)
      )
      (drop_path): timm.DropPath(0.10000000149011612)
      (conv33conv33conv11): Sequential(
        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU()
        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
        (8): ReLU()
      )
    )
  )
)
layers.3.blocks -> ModuleList(
  (0): SS_Conv_SSM(
    (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=384, out_features=1536, bias=False)
      (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      (act): SiLU()
      (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=768, out_features=384, bias=False)
    )
    (drop_path): timm.DropPath(0.08888889104127884)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
  (1): SS_Conv_SSM(
    (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (self_attention): SS2D(
      (in_proj): Linear(in_features=384, out_features=1536, bias=False)
      (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      (act): SiLU()
      (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (out_proj): Linear(in_features=768, out_features=384, bias=False)
    )
    (drop_path): timm.DropPath(0.10000000149011612)
    (conv33conv33conv11): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (8): ReLU()
    )
  )
)
layers.3.blocks.0 -> SS_Conv_SSM(
  (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=384, out_features=1536, bias=False)
    (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
    (act): SiLU()
    (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=768, out_features=384, bias=False)
  )
  (drop_path): timm.DropPath(0.08888889104127884)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.3.blocks.0.ln_1 -> LayerNorm((384,), eps=1e-05, elementwise_affine=True)
layers.3.blocks.0.self_attention -> SS2D(
  (in_proj): Linear(in_features=384, out_features=1536, bias=False)
  (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
  (act): SiLU()
  (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=768, out_features=384, bias=False)
)
layers.3.blocks.0.self_attention.in_proj -> Linear(in_features=384, out_features=1536, bias=False)
layers.3.blocks.0.self_attention.conv2d -> Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
layers.3.blocks.0.self_attention.act -> SiLU()
layers.3.blocks.0.self_attention.out_norm -> LayerNorm((768,), eps=1e-05, elementwise_affine=True)
layers.3.blocks.0.self_attention.out_proj -> Linear(in_features=768, out_features=384, bias=False)
layers.3.blocks.0.drop_path -> timm.DropPath(0.08888889104127884)
layers.3.blocks.0.conv33conv33conv11 -> Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.3.blocks.0.conv33conv33conv11.0 -> BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.3.blocks.0.conv33conv33conv11.1 -> Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.3.blocks.0.conv33conv33conv11.2 -> BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.3.blocks.0.conv33conv33conv11.3 -> ReLU()
layers.3.blocks.0.conv33conv33conv11.4 -> Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.3.blocks.0.conv33conv33conv11.5 -> BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.3.blocks.0.conv33conv33conv11.6 -> ReLU()
layers.3.blocks.0.conv33conv33conv11.7 -> Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
layers.3.blocks.0.conv33conv33conv11.8 -> ReLU()
layers.3.blocks.1 -> SS_Conv_SSM(
  (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (self_attention): SS2D(
    (in_proj): Linear(in_features=384, out_features=1536, bias=False)
    (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
    (act): SiLU()
    (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=768, out_features=384, bias=False)
  )
  (drop_path): timm.DropPath(0.10000000149011612)
  (conv33conv33conv11): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    (8): ReLU()
  )
)
layers.3.blocks.1.ln_1 -> LayerNorm((384,), eps=1e-05, elementwise_affine=True)
layers.3.blocks.1.self_attention -> SS2D(
  (in_proj): Linear(in_features=384, out_features=1536, bias=False)
  (conv2d): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
  (act): SiLU()
  (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=768, out_features=384, bias=False)
)
layers.3.blocks.1.self_attention.in_proj -> Linear(in_features=384, out_features=1536, bias=False)
layers.3.blocks.1.self_attention.conv2d -> Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
layers.3.blocks.1.self_attention.act -> SiLU()
layers.3.blocks.1.self_attention.out_norm -> LayerNorm((768,), eps=1e-05, elementwise_affine=True)
layers.3.blocks.1.self_attention.out_proj -> Linear(in_features=768, out_features=384, bias=False)
layers.3.blocks.1.drop_path -> timm.DropPath(0.10000000149011612)
layers.3.blocks.1.conv33conv33conv11
-> Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU()
  (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU()
  (7): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
  (8): ReLU()
)
layers.3.blocks.1.conv33conv33conv11.0 -> BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.3.blocks.1.conv33conv33conv11.1 -> Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.3.blocks.1.conv33conv33conv11.2 -> BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.3.blocks.1.conv33conv33conv11.3 -> ReLU()
layers.3.blocks.1.conv33conv33conv11.4 -> Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
layers.3.blocks.1.conv33conv33conv11.5 -> BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
layers.3.blocks.1.conv33conv33conv11.6 -> ReLU()
layers.3.blocks.1.conv33conv33conv11.7 -> Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
layers.3.blocks.1.conv33conv33conv11.8 -> ReLU()
avgpool -> AdaptiveAvgPool2d(output_size=1)
head -> Linear(in_features=768, out_features=2, bias=True)
edge_extractor -> SobelConv(
  (sobel_conv_y): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)
  (sobel_conv_x): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)
)
edge_extractor.sobel_conv_y -> Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)
edge_extractor.sobel_conv_x -> Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)
edge_pools -> ModuleList(
  (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (1): MaxPool2d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)
  (2): MaxPool2d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
)
edge_pools.0 -> MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
edge_pools.1 -> MaxPool2d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)
edge_pools.2 -> MaxPool2d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
edge_convs -> ModuleList(
  (0): Conv(
    (conv): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): SiLU()
  )
  (1): Conv(
    (conv): Conv2d(3, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): SiLU()
  )
  (2): Conv(
    (conv): Conv2d(3, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): SiLU()
  )
)
edge_convs.0 -> Conv(
  (conv): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act): SiLU()
)
edge_convs.0.conv -> Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
edge_convs.0.bn -> BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
edge_convs.0.act -> SiLU()
edge_convs.1 -> Conv(
  (conv): Conv2d(3, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act): SiLU()
)
edge_convs.1.conv -> Conv2d(3, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
edge_convs.1.bn -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
edge_convs.2 -> Conv(
  (conv): Conv2d(3, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act): SiLU()
)
edge_convs.2.conv -> Conv2d(3, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
edge_convs.2.bn -> BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
fusers -> ModuleList(
  (0): GatedConvEdgeFusion(
    (align_main): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
    (align_edge): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
    (gate): Sequential(
      (0): Conv2d(192, 2, kernel_size=(1, 1), stride=(1, 1))
      (1): Softmax(dim=1)
    )
    (output_proj): Sequential(
      (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (1): GatedConvEdgeFusion(
    (align_main): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    (align_edge): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    (gate): Sequential(
      (0): Conv2d(384, 2, kernel_size=(1, 1), stride=(1, 1))
      (1): Softmax(dim=1)
    )
    (output_proj): Sequential(
      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (2): GatedConvEdgeFusion(
    (align_main): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    (align_edge): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    (gate): Sequential(
      (0): Conv2d(768, 2, kernel_size=(1, 1), stride=(1, 1))
      (1): Softmax(dim=1)
    )
    (output_proj): Sequential(
      (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
)
fusers.0 -> GatedConvEdgeFusion(
  (align_main): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
  (align_edge): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
  (gate): Sequential(
    (0): Conv2d(192, 2, kernel_size=(1, 1), stride=(1, 1))
    (1): Softmax(dim=1)
  )
  (output_proj): Sequential(
    (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
)
fusers.0.align_main -> Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
fusers.0.align_edge -> Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
fusers.0.gate -> Sequential(
  (0): Conv2d(192, 2, kernel_size=(1, 1), stride=(1, 1))
  (1): Softmax(dim=1)
)
fusers.0.gate.0 -> Conv2d(192, 2, kernel_size=(1, 1), stride=(1, 1))
fusers.0.gate.1 -> Softmax(dim=1)
fusers.0.output_proj -> Sequential(
  (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
  (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
fusers.0.output_proj.0 -> Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
fusers.0.output_proj.1 -> BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
fusers.0.output_proj.2 -> ReLU(inplace=True)
fusers.1 -> GatedConvEdgeFusion(
  (align_main): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
  (align_edge): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
  (gate): Sequential(
    (0): Conv2d(384, 2, kernel_size=(1, 1), stride=(1, 1))
    (1): Softmax(dim=1)
  )
  (output_proj): Sequential(
    (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
)
fusers.1.align_main -> Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
fusers.1.align_edge -> Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
fusers.1.gate -> Sequential(
  (0): Conv2d(384, 2, kernel_size=(1, 1), stride=(1, 1))
  (1): Softmax(dim=1)
)
fusers.1.gate.0 -> Conv2d(384, 2, kernel_size=(1, 1), stride=(1, 1))
fusers.1.gate.1 -> Softmax(dim=1)
fusers.1.output_proj -> Sequential(
  (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
  (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
fusers.1.output_proj.0 -> Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
fusers.1.output_proj.1 -> BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
fusers.1.output_proj.2 -> ReLU(inplace=True)
fusers.2 -> GatedConvEdgeFusion(
  (align_main): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
  (align_edge): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
  (gate): Sequential(
    (0): Conv2d(768, 2, kernel_size=(1, 1), stride=(1, 1))
    (1): Softmax(dim=1)
  )
  (output_proj): Sequential(
    (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
)
fusers.2.align_main -> Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
fusers.2.align_edge -> Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
fusers.2.gate -> Sequential(
  (0): Conv2d(768, 2, kernel_size=(1, 1), stride=(1, 1))
  (1): Softmax(dim=1)
)
fusers.2.gate.0 -> Conv2d(768, 2, kernel_size=(1, 1), stride=(1, 1))
fusers.2.gate.1 -> Softmax(dim=1)
fusers.2.output_proj -> Sequential(
  (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
  (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
fusers.2.output_proj.0 -> Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
fusers.2.output_proj.1 -> BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
fusers.2.output_proj.2 -> ReLU(inplace=True)