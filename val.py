# -*- coding: utf-8 -*-
"""
å¯¹ MedMamba ç³»åˆ—æ¨¡å‹è¿›è¡Œ K-Fold éªŒè¯
ä¿æŒä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´çš„è¯„ä¼°æ–¹å¼ï¼š
    - accuracy_score
    - precision_recall_fscore_support(average='macro')
ä¸ä¿®æ”¹ä»»ä½•æŒ‡æ ‡å®šä¹‰
"""
import os
import sys
import json
import pickle
import argparse
import logging
import numpy as np

import torch
from torch.utils.data import DataLoader, Subset
from torchvision import transforms, datasets
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# å¯¼å…¥æ¨¡å‹ï¼ˆç¡®ä¿ MedMamba.py åœ¨è·¯å¾„ä¸­ï¼‰
from MedMamba import VSSM as vssm
from MedMamba import VSSMEdgeEnhanced as edge_enhanced
from MedMamba import DualBranchVSSM as dual_branch
from MedMamba import DualBranchVSSMEnhanced as dual_branch_enhanced

# æ¨¡å‹æ˜ å°„
MODEL_MAP = {
    'vssm': vssm,
    'edge_enhanced': edge_enhanced,
    'dual_branch': dual_branch,
    'dual_branch_enhanced': dual_branch_enhanced,
}

# ========== Utils ==========
def seed_everything(seed=42):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def setup_logger(log_dir):
    log_file = os.path.join(log_dir, "evaluate_log.txt")
    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s][%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logging.info(f"Evaluation log saved to: {log_file}")


@torch.no_grad()
def evaluate_with_metrics(model, loader, device, num_classes):
    """
    ä½¿ç”¨ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´çš„æ–¹å¼è®¡ç®—æŒ‡æ ‡ï¼š
        - Accuracy: accuracy_score
        - Precision, Recall, F1: average='macro'
    """
    model.eval()
    all_preds = []
    all_labels = []

    pbar = tqdm(loader, desc="Evaluating", leave=False, file=sys.stdout)
    for images, labels in pbar:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        preds = outputs.argmax(dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)

    # âœ… å®Œå…¨å¤ç°åŸé€»è¾‘
    acc = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='macro', zero_division=0
    )

    return {
        'acc': float(acc),
        'precision': float(precision),
        'recall': float(recall),
        'f1': float(f1)
    }


# ========== Main ==========
def main():
    parser = argparse.ArgumentParser(description="Evaluate MedMamba K-Fold Models (Original Metric Logic)")
    parser.add_argument('--log_dir', type=str, required=True,
                        help='Path to the training log directory (contains best models)')
    parser.add_argument('--dataset', type=str, default='å•ä¸ªç»†èƒåˆ†ç±»æ•°æ®é›†äºŒåˆ†ç±»S2L',
                        help='Dataset name under /data/')
    parser.add_argument('--model_type', type=str, required=True,
                        choices=['vssm', 'edge_enhanced', 'dual_branch', 'dual_branch_enhanced'],
                        help='Model type used in training')
    parser.add_argument('--num_classes', type=int, default=2)
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--seed', type=int, default=42)

    # æ¨¡å‹ç»“æ„å‚æ•°ï¼ˆéœ€ä¸è®­ç»ƒä¸€è‡´ï¼‰
    parser.add_argument('--edge_layer_idx', type=int, default=0)
    parser.add_argument('--fusion_levels', nargs='+', type=int, default=[1, 2])
    parser.add_argument('--edge_attention', type=str, default='none', choices=['none', 'se', 'cbam'])
    parser.add_argument('--fusion_mode', type=str, default='concat', choices=['concat', 'gate', 'dual'])

    args = parser.parse_args()
    seed_everything(args.seed)

    # è®¾å¤‡
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # æ—¥å¿—
    setup_logger(args.log_dir)

    # æ•°æ®é›†è·¯å¾„
    dataset_root = f"/data/{args.dataset}"
    full_dataset = datasets.ImageFolder(root=dataset_root, transform=None)
    logging.info(f"Loaded dataset from: {dataset_root}, total samples: {len(full_dataset)}")

    # ================================
    # åŠ è½½ kfold_splits.pkl
    # ================================
    kfold_pkl_path = os.path.join(dataset_root, "kfold_splits.pkl")
    if not os.path.exists(kfold_pkl_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°åˆ’åˆ†æ–‡ä»¶: {kfold_pkl_path}")

    logging.info(f"âœ… åŠ è½½åˆ’åˆ†æ–‡ä»¶: {kfold_pkl_path}")
    with open(kfold_pkl_path, 'rb') as f:
        data = pickle.load(f)

    splits = data['splits']
    fold_data = []

    # æ„å»ºè·¯å¾„ â†’ ç´¢å¼• æ˜ å°„
    path_to_idx = {
        os.path.join(dataset_root, img).replace("\\", "/").replace("//", "/"): idx
        for idx, (img, _) in enumerate(full_dataset.imgs)
    }

    for fold_idx, split in enumerate(splits):
        rel_train = split['train']
        rel_val = split['val']
        mean = split['mean']
        std = split['std']

        full_train = [os.path.join(dataset_root, p).replace("\\", "/") for p in rel_train]
        full_val = [os.path.join(dataset_root, p).replace("\\", "/") for p in rel_val]

        train_idx = [path_to_idx[p] for p in full_train if p in path_to_idx]
        val_idx = [path_to_idx[p] for p in full_val if p in path_to_idx]

        if len(train_idx) == 0 or len(val_idx) == 0:
            raise ValueError(f"Fold {fold_idx + 1} æ•°æ®ä¸ºç©º")

        fold_data.append({
            'train_idx': train_idx,
            'val_idx': val_idx,
            'mean': mean,
            'std': std
        })

    logging.info(f"âœ… æˆåŠŸåŠ è½½ {len(fold_data)} æŠ˜åˆ’åˆ†")

    # ================================
    # å¼€å§‹éªŒè¯æ¯æŠ˜æ¨¡å‹
    # ================================
    fold_results = []

    for fold, data in enumerate(fold_data, start=1):
        print(f"\n========== Evaluating Fold {fold} ==========")
        val_idx = data['val_idx']
        mean = data['mean']
        std = data['std']
        mean = 0.5
        std = 0.5
        # æ•°æ®å˜æ¢
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean, std=std)
        ])

        dataset = datasets.ImageFolder(root=dataset_root, transform=transform)
        val_subset = Subset(dataset, val_idx)
        val_loader = DataLoader(val_subset, batch_size=args.batch_size, shuffle=False,
                                num_workers=args.num_workers, pin_memory=True)

        # æ„å»ºæ¨¡å‹
        model_class = MODEL_MAP[args.model_type]
        model_kwargs = {}

        if args.model_type == 'edge_enhanced':
            model_kwargs.update({
                'edge_layer_idx': args.edge_layer_idx,
                'fusion_levels': args.fusion_levels,
                'edge_attention': args.edge_attention,
                'fusion_mode': args.fusion_mode,
            })
        elif args.model_type in ['dual_branch', 'dual_branch_enhanced']:
            model_kwargs.update({
                'fusion_levels': args.fusion_levels,
                'edge_attention': args.edge_attention,
                'fusion_mode': args.fusion_mode,
            })

        model = model_class(
            depths=[2, 2, 4, 2],
            dims=[96, 192, 384, 768],
            num_classes=args.num_classes,
            **model_kwargs
        ).to(device)

        # åŠ è½½æƒé‡
        model_path = os.path.join(args.log_dir, f"UCNet_fold{fold}_best.pth")
        if not os.path.exists(model_path):
            logging.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            continue

        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict)
        logging.info(f"âœ… åŠ è½½æ¨¡å‹æƒé‡: {model_path}")

        # è¯„ä¼°
        metrics = evaluate_with_metrics(model, val_loader, device, args.num_classes)
        fold_results.append(metrics)

        # æ‰“å°ç»“æœ
        print(f"ğŸ“Œ Fold {fold} Results:")
        print(f"    Accuracy : {metrics['acc']:.4f}")
        print(f"    Precision: {metrics['precision']:.4f}")
        print(f"    Recall   : {metrics['recall']:.4f}")
        print(f"    F1       : {metrics['f1']:.4f}")

    # ================================
    # æ±‡æ€»ç»“æœ
    # ================================
    if not fold_results:
        logging.error("âŒ æœªè¯„ä¼°ä»»ä½• foldï¼Œæ£€æŸ¥æ¨¡å‹è·¯å¾„")
        return

    all_acc = [r['acc'] for r in fold_results]
    all_prec = [r['precision'] for r in fold_results]
    all_rec = [r['recall'] for r in fold_results]
    all_f1 = [r['f1'] for r in fold_results]

    summary = {
        "Average Accuracy": float(np.mean(all_acc)),
        "Std Accuracy": float(np.std(all_acc)),
        "Average Precision": float(np.mean(all_prec)),
        "Average Recall": float(np.mean(all_rec)),
        "Average F1": float(np.mean(all_f1)),
        "Per-fold Results": [
            {
                "fold": i + 1,
                "acc": r['acc'],
                "precision": r['precision'],
                "recall": r['recall'],
                "f1": r['f1']
            }
            for i, r in enumerate(fold_results)
        ]
    }

    print("\n========== Final Cross-Validation Evaluation ==========")
    for k, v in summary.items():
        if k != "Per-fold Results":
            print(f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}")

    # ä¿å­˜ç»“æœ
    summary_path = os.path.join(args.log_dir, "evaluation_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=4, ensure_ascii=False)

    logging.info(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {summary_path}")


if __name__ == '__main__':
    main()